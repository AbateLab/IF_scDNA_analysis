{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a233039e",
   "metadata": {},
   "source": [
    "## Install packages & dependecies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d98630db",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "%%bash\n",
    "#Cyrille's github\n",
    "git clone https://github.com/cdelley/scyBCB\n",
    "pip install git+https://github.com/cdelley/scyBCB\n",
    "\n",
    "#Install python dependencies\n",
    "pip install watermark\n",
    "pip install anndata\n",
    "pip install \"numpy<2\"\n",
    "pip install xopen\n",
    "pip install PyQt5\n",
    "\n",
    "#Install GATK 4.1.3.0 because Java on cluster is 11 and only compatible with GATK 4.1.x.x.\n",
    "!unzip /home/caleb/scDNA_analysis/gatk-4.1.3.0.zip\n",
    "\n",
    "#Install bwa-mem2 (Linux)\n",
    "!curl -L https://github.com/bwa-mem2/bwa-mem2/releases/download/v2.2.1/bwa-mem2-2.2.1_x64-linux.tar.bz2 \\\n",
    "  | tar jxf -\n",
    "  \n",
    "#Install yaml\n",
    "!pip install pyyaml\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "087f5b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a29102",
   "metadata": {},
   "source": [
    "## Import libraries & Tool setup:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "219cadd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the github repo\n",
    "import scyBCB.CyGeno as dabm\n",
    "import scyBCB.CyBCB as bcb\n",
    "\n",
    "# Install libraries per conda instruction as needed: umap, csv, etc. \n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import subprocess\n",
    "import gzip\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import subprocess\n",
    "import yaml\n",
    "import re\n",
    "import csv\n",
    "import scanpy as sc\n",
    "\n",
    "#matplotlib.use('Qt5Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "matplotlib.use('Agg')\n",
    "%matplotlib inline\n",
    "\n",
    "%load_ext watermark\n",
    "%watermark -v -iv\n",
    "\n",
    "from collections import Counter\n",
    "from multiprocessing.pool import ThreadPool\n",
    "from multiprocessing import Process\n",
    "\n",
    "import umap\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import seaborn as sns\n",
    "import json\n",
    "from sklearn.cluster import KMeans\n",
    "import dask.dataframe as dd\n",
    "from multiprocessing import Pool\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "import fastcluster\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b89597c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load YAML configuration file\n",
    "with open('config_example.yaml', 'r') as file:\n",
    "    config = yaml.safe_load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98bb9ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105b81a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!chmod +x {config['demuxbyname_path']}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "403fc9d6",
   "metadata": {},
   "source": [
    "## File/Folder directories (change here):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e33c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Barcode whitelist help to calculate cell barcode base offset (1-3 nucleotide btw BC1 and BC2) if beads are synthesized in split-pool approaches.\n",
    "barcodes = [config['barcode_whitelist_1'],\n",
    "            config['barcode_whitelist_2']]\n",
    "bc_dicts = []\n",
    "for bc_json in barcodes:\n",
    "    with open(bc_json, 'r') as fin:\n",
    "        bc_dicts.append(json.load(fin))\n",
    "        \n",
    "# Forward and reverse primer .csv files of the targeted DNA panels (AML or ML):\n",
    "primers = pd.read_csv(config['primer_csv'])\n",
    "primer_dict_fw = {s[:19]:n+'_fw' for (s,n) in zip(list(primers['fwd_seq']), list(primers['AmpID']))}\n",
    "primer_dict_rev = {s[:19]:n+'_rev' for (s,n) in zip(list(primers['rev_seq']), list(primers['AmpID']))}\n",
    "\n",
    "# Directory to Read 1 and Read 2 files\n",
    "f1 = config['read_1_file']\n",
    "f2 = config['read_2_file']\n",
    "\n",
    "# Output folder (to dump processed sequencing file in)\n",
    "out_folder = config['fastq_out_folder']\n",
    "os.makedirs(out_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "219e4ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Barcode template:\n",
    "# MissionBio beads\n",
    "MB_seq_pos_dict = {\n",
    "    'bc'  : [(0,0,9), (0,23,32)], # two barcodes\n",
    "    'feature' : [(0,47,66), (1,0,19)],        # features, in this case, are the primer positions for stats, normally this is to indicate antibody barcode tags\n",
    "    'seq': [(0,66,151), (1,0,151)],           # sequence positions including the primer region. These are copied to the fastq output\n",
    "}\n",
    "\n",
    "# Lab-made beads:\n",
    "LM_seq_pos_dict = {\n",
    "    'bc'  : [(0,0,11), (0,12,20), (0,24,32)], # three barcodes\n",
    "    'feature' : [(0,51,70), (1,0,19)],        \n",
    "    'seq': [(0,51,151), (1,0,151)],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ebc82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bwa-mem2 index location\n",
    "bwamem2 = config['bwamem2']\n",
    "# folder directories for bam and vcf file\n",
    "bam_dir = f\"{out_folder}/bam/\"\n",
    "bam_with_RG_dir = f\"{out_folder}/bam_with_RG/\"\n",
    "vcf_dir = f\"{out_folder}/vcf/\"\n",
    "vcf_csv_dir = f\"{out_folder}/vcf_csv/\"\n",
    "os.makedirs(bam_dir, exist_ok=True)\n",
    "os.makedirs(bam_with_RG_dir, exist_ok=True)\n",
    "os.makedirs(vcf_dir, exist_ok=True)\n",
    "\n",
    "# reference genome file and primer location for variant callling\n",
    "human_fasta_file = config['human_fastq_file']\n",
    "interval_file = config['interval_file'] #only look at amplicons that contain GM1 & GM2 variants"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ece69ee",
   "metadata": {},
   "source": [
    "## Filter reads based on cell barcode & fwd/rvs primer alignment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b346d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "exp = dabm.sequences.write_good_reads(\n",
    "    file1=f1,\n",
    "    file2=f2, \n",
    "    bc_correction_dic=bc_dicts,\n",
    "    primer_dic=[primer_dict_fw, primer_dict_rev],\n",
    "    out_path=out_folder,\n",
    "    seq_positions=MB_seq_pos_dict,\n",
    "    verbose=True, \n",
    "    #down_sample=10**3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18674709",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recover_barcode(barcode_count_dict, low_count_filter=200, batch_label=''):\n",
    "    \"\"\"\n",
    "    This functions filter barcodes based on the total number of reads per barcode.\n",
    "    The default minimum total of reads required is 200.\n",
    "    barcode_count_dict: dictionaries from write_good_reads function where each key is the barcode, each value is the total read counts\n",
    "    low_count_filter: minimum of reads per barcode to be selected for downstream processing\n",
    "    \"\"\"\n",
    "    \n",
    "    # count reads per barcode\n",
    "    reads = np.array(list(barcode_count_dict.values()))\n",
    "    _thresh_idx = reads >= low_count_filter\n",
    "\n",
    "    bcs = np.array(list(barcode_count_dict.keys()))[_thresh_idx]  \n",
    "    cell_barcodes = np.sort(bcs)\n",
    "\n",
    "    barcode_count_touples = list(barcode_count_dict.items())\n",
    "    return cell_barcodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c0582ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "barcode_recover = recover_barcode(exp.bc_groups)\n",
    "barcode_string = barcode_recover.astype(str)\n",
    "np.savetxt(out_folder + \"/filtered_barcode_string.txt\", barcode_string, fmt='%s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136174c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# demultiplex the R1 and R2 into multiple fastq.gz file for each barcode\n",
    "with open(f'{out_folder}/barcode_recover', 'w') as fout:\n",
    "    for i in barcode_recover:\n",
    "        fout.write(str(i)+',')\n",
    "\n",
    "# Change here for path to bbmap\n",
    "_pp1 = f\"{config['demuxbyname_path']} -Xmx60g \"\n",
    "_pp2 = f\"in={out_folder}/R1.fastq.gz in2={out_folder}/R2.fastq.gz out={out_folder}/demuxed_all/cell_%.fastq.gz  \"\n",
    "_pp3 = f\"delimiter=_ suffixmode=t names={out_folder}/filtered_barcode_string.txt\"\n",
    "\n",
    "cmd = _pp1+_pp2+_pp3\n",
    "subprocess.run(cmd, shell = True, executable=\"/bin/bash\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c36136d",
   "metadata": {},
   "source": [
    "## Target amplicon read counts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "019eb7c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fastq_to_txt(input_folder, output_folder):\n",
    "    \"\"\"\n",
    "    This function is to extract R2 from .fastq.gz files into txt files\n",
    "    We can cut down this function somehow in the future to increase speed of the pipeline\n",
    "    input_folder: folder containing .fastq.gz files\n",
    "    output_folder: folder containing .txt files (that are converted from .fastq.gz files)\n",
    "    \"\"\"\n",
    "    # Ensure output folder exists\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    # Regular expression to capture the sequence after \"-R2-\" up to the underscore\n",
    "    pattern = re.compile(r\"@.*-R2-([A-Z]+)\")\n",
    "\n",
    "    # Loop through each FASTQ file in the input folder\n",
    "    for filename in os.listdir(input_folder):\n",
    "        if filename.endswith(\".fastq.gz\"):\n",
    "            input_path = os.path.join(input_folder, filename)\n",
    "            output_filename = filename.replace(\".fastq.gz\", \"_R2_sequences.txt\")\n",
    "            output_path = os.path.join(output_folder, output_filename)\n",
    "\n",
    "            # Open the output file to write extracted sequences\n",
    "            with open(output_path, \"w\") as output_file:\n",
    "                # Open and read the compressed FASTQ file\n",
    "                with gzip.open(input_path, \"rt\") as fastq_file:\n",
    "                    for line in fastq_file:\n",
    "                        # Check if the line is a header with \"-R2-\"\n",
    "                        match = pattern.match(line)\n",
    "                        if match:\n",
    "                            # Write the extracted sequence to the output file\n",
    "                            sequence_after_R2 = match.group(1)\n",
    "                            output_file.write(sequence_after_R2 + \"\\n\")\n",
    "\n",
    "            print(f\"Processed {filename} and saved to {output_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee0075b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fastq_to_txt(out_folder+\"/demuxed_all\", out_folder+\"/R2_txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b0657e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start counting number of reads align to each amplicon per R2 values\n",
    "R2_txt_folder = out_folder + \"/R2_txt\"\n",
    "# Initialize an empty DataFrame to store the counts\n",
    "count_table = pd.DataFrame(columns=primer_dict_rev.values())\n",
    "\n",
    "# Loop through each .txt file in the output directory\n",
    "for filename in os.listdir(R2_txt_folder):\n",
    "    if filename.endswith(\"_R2_sequences.txt\"):\n",
    "        file_path = os.path.join(R2_txt_folder, filename)\n",
    "\n",
    "        # Count occurrences where a sequence aligns with any target primer\n",
    "        with open(file_path, \"r\") as f:\n",
    "            sequences = f.read().splitlines()\n",
    "            file_counts = {target_name: 0 for target_name in primer_dict_rev.values()}\n",
    "\n",
    "            for sequence in sequences:\n",
    "                for target_seq, target_name in primer_dict_rev.items():\n",
    "                    # Check if target sequence is a substring of the read sequence\n",
    "                    if sequence in target_seq:\n",
    "                        file_counts[target_name] += 1\n",
    "\n",
    "        # Add the counts as a new row in the DataFrame, with the file name as the index\n",
    "        count_table.loc[filename] = file_counts\n",
    "\n",
    "# Display the final table\n",
    "count_table.index.name = \"File\"\n",
    "count_table.index = count_table.index.str.split('_R2').str[0]\n",
    "count_table.to_csv(out_folder + \"/target_amplicon_counts.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76ee10fd",
   "metadata": {},
   "source": [
    "## Read-count related metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "262159af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gini_coefficient(read_counts):\n",
    "    \"\"\"    \n",
    "    This function calculates the Gini coefficient per barcode\n",
    "    \n",
    "    Parameters:\n",
    "    read_counts (list or numpy array): A list or array of read counts per barcode\n",
    "    \n",
    "    Returns:\n",
    "    float: The Shannon entropy per barcode.\n",
    "    \"\"\"\n",
    "    read_counts = sorted(read_counts)\n",
    "    n = len(read_counts)\n",
    "    total_read_counts = sum(read_counts)\n",
    "    if total_read_counts == 0:\n",
    "        return 0\n",
    "    gini_sum = sum((2 * (i+1) - n - 1) * read_counts[i] for i in range(n))\n",
    "    gini = gini_sum / (n * total_read_counts)\n",
    "    return gini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da702bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shannon_entropy(read_counts):\n",
    "    \"\"\"\n",
    "    Calculate Shannon Entropy for a set of read counts, handling zero read counts appropriately.\n",
    "    \n",
    "    Parameters:\n",
    "    read_counts (list or numpy array): A list or array of read counts per barcode\n",
    "    \n",
    "    Returns:\n",
    "    float: The Shannon entropy per barcode.\n",
    "    \"\"\"\n",
    "    # Convert read counts to a numpy array for easier handling\n",
    "    read_counts = np.array(read_counts)\n",
    "    \n",
    "    # Normalize the read counts to obtain probabilities (p_i)\n",
    "    total_reads = np.sum(read_counts)\n",
    "    \n",
    "    probabilities = read_counts / total_reads\n",
    "    \n",
    "    # Calculate Shannon entropy, ignoring zero probabilities\n",
    "    entropy = 0.0\n",
    "    for p in probabilities:\n",
    "        if p > 0:  # Ignore terms where p_i is zero\n",
    "            entropy -= p * np.log2(p)\n",
    "    \n",
    "    return entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f51fcbe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fraction_amplicons_above_threshold(read_counts, threshold):\n",
    "    \"\"\"\n",
    "    This function calculates the fraction of amplicons with read counts greater than or equal to the threshold.\n",
    "    \n",
    "    Parameters:\n",
    "    read_counts (list): A list of read counts for each amplicon.\n",
    "    threshold (int or float): The threshold value for filtering read counts.\n",
    "    \n",
    "    Returns:\n",
    "    float: Fraction of amplicons with read counts >= threshold.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Total number of amplicons\n",
    "    total_amplicons = len(read_counts)\n",
    "    \n",
    "    # Avoid division by zero if there are no amplicons\n",
    "    if total_amplicons == 0:\n",
    "        return 0\n",
    "    \n",
    "    # Count the number of amplicons with reads >= threshold\n",
    "    amplicons_above_threshold = sum(1 for count in read_counts if count >= threshold)\n",
    "    \n",
    "    # Calculate the fraction\n",
    "    fraction = amplicons_above_threshold / total_amplicons\n",
    "    \n",
    "    return fraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87618ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate metrics and compile into the target_amplicon_counts.csv\n",
    "total_reads = count_table.sum(axis=1)\n",
    "Gini = count_table.apply(lambda row: gini_coefficient(row), axis=1)\n",
    "evenness = 1 - Gini\n",
    "entropy = count_table.apply(lambda row: shannon_entropy(row), axis=1)\n",
    "x1 = count_table.apply(lambda row: fraction_amplicons_above_threshold(row.tolist(), 1), axis=1)\n",
    "x10 = count_table.apply(lambda row: fraction_amplicons_above_threshold(row.tolist(), 10), axis=1)\n",
    "x20 = count_table.apply(lambda row: fraction_amplicons_above_threshold(row.tolist(), 20), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61930700",
   "metadata": {},
   "outputs": [],
   "source": [
    "read_count_quality_metrics = pd.DataFrame({'Total_reads': total_reads,\n",
    "                                          'Evenness': evenness,\n",
    "                                          'Entropy': entropy,\n",
    "                                          'x1': x1,\n",
    "                                          'x10': x10,\n",
    "                                          'x20': x20})\n",
    "read_count_quality_metrics.to_csv(out_folder + \"/read_count_quality_metrics.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f7065ce",
   "metadata": {},
   "source": [
    "## Filter cells based on read count quality metrics\n",
    "Currently: x10 > 0.8 and Evenness > 0.5 but can be modified based on config values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd46f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "x10_threshold = config['x10_threshold']\n",
    "evenness_threshold = config['evenness_threshold']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a80d663b",
   "metadata": {},
   "outputs": [],
   "source": [
    "read_count_quality_metrics = pd.read_csv(out_folder + \"/read_count_quality_metrics.csv\")\n",
    "cell_pass_filter = read_count_quality_metrics[(read_count_quality_metrics['x10'] >= 0.8) & (read_count_quality_metrics['Evenness'] >= 0.5)]['File'].str.replace('cell_', '', regex=False).to_numpy()\n",
    "np.savetxt(out_folder + \"/filtered_cells_based_on_x10_evenness.txt\", cell_pass_filter, fmt=\"%s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "470c29b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = out_folder + \"/filtered_cells_based_on_x10_evenness.txt\"\n",
    "with open(file_path, 'r') as file:\n",
    "    barcode_recover = file.readlines()\n",
    "barcode_recover = [line.strip() for line in barcode_recover]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0715d588",
   "metadata": {},
   "source": [
    "## Genome alignment, Variant calling & Cell line analysis:\n",
    "This code is optimized towards 2 cell lines analysis **ONLY**\n",
    "If you need different cell lines than GM1 and GM2, please contact Caleb @ caleb_thinh_tong@berkeley.edu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e97100",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SingleCell(object):\n",
    "    # class for storing metadata for each single cell file\n",
    "\n",
    "    def __init__(self, cell_barcode, out_folder, bam_dir, bam_with_RG_dir, vcf_dir):\n",
    "        # initialize object by generating filenames\n",
    "\n",
    "        self.cell_barcode = cell_barcode                    # cell barcode\n",
    "        self.fastq = f\"{out_folder}demuxed_all/cell_{cell_barcode}.fastq.gz\"     # fastq file\n",
    "\n",
    "        self.bam = bam_dir + \"cell_\" + cell_barcode + '.bam'          # bam file\n",
    "        self.bai = bam_dir + \"cell_\" + cell_barcode + '.bai'          # bam file index\n",
    "        self.bam_with_RG = bam_with_RG_dir + \"cell_\" + cell_barcode + '.bam'          # bam file\n",
    "        self.bai_with_RG = bam_with_RG_dir + \"cell_\" + cell_barcode + '.bai'          # bam file index\n",
    "        self.vcf = vcf_dir + \"cell_\" + cell_barcode + '.g.vcf'        # gvcf file\n",
    "\n",
    "        self.valid = False      # marker for valid cells\n",
    "        self.alignments = {}    # alignment counts for each interval\n",
    "\n",
    "    def align_and_index(self, ref_idx, align_cmd = 'bwa-mem2'):\n",
    "\n",
    "        # align the panel to the bowtie2 human index and generate sorted bam file\n",
    "        if align_cmd == 'bwa-mem2':\n",
    "            align_cmd = f\"{config['bwamem2_path']} mem -t1\" \\\n",
    "                        f' {ref_idx:s} {self.fastq:s}'\n",
    "        elif align_cmd == 'minimap2':\n",
    "            pass\n",
    "        elif align_cmd == 'bowtie2':\n",
    "            align_cmd = f'bowtie2 -x {ref_idx} --mm --interleaved {self.fastq} --rg-id {self.cell_barcode} ' \\\n",
    "                        f'--rg SM:{self.cell_barcode} --rg PL:ILLUMINA --rg CN:UCSF --quiet'\n",
    "        else:\n",
    "            align_cmd = align_cmd\n",
    "\n",
    "        # filter and sort the output\n",
    "        align_cmd = align_cmd + f\"| {config['samtools_path']} view -b -q 3 -F 4 -F 0X0100 | {config['samtools_path']} sort -o {self.bam:s}\"\n",
    "        \n",
    "        #print(align_cmd)\n",
    "        subprocess.call(align_cmd, shell=True)\n",
    "\n",
    "        # index all bam files using samtools\n",
    "        index_cmd = f\"{config['samtools_path']} index {self.bam} {self.bai}\"\n",
    "        subprocess.call(index_cmd, shell=True)\n",
    "\n",
    "    def call_variants(self, fasta, interval_file):\n",
    "        # call variants using gatk\n",
    "\n",
    "        variants_cmd = f\"python3 {config['gatk_path']} HaplotypeCaller \" \\\n",
    "                       f'-R %s -I %s -O %s -L %s ' \\\n",
    "                       f'--verbosity ERROR ' \\\n",
    "                       f'--native-pair-hmm-threads 1 ' \\\n",
    "                       f'--standard-min-confidence-threshold-for-calling 0 ' \\\n",
    "                       f'--emit-ref-confidence GVCF ' \\\n",
    "                       f'--max-reads-per-alignment-start 0 ' \\\n",
    "                       f'--max-alternate-alleles 2 ' \\\n",
    "                       f'--minimum-mapping-quality 0' \\\n",
    "                       % (fasta,\n",
    "                          self.bam_with_RG,\n",
    "                          self.vcf,\n",
    "                          interval_file)\n",
    "\n",
    "        #print(variants_cmd)\n",
    "        #process = subprocess.Popen(variants_cmd, shell=True)\n",
    "        subprocess.call(variants_cmd, shell=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7376a82f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cells = [\n",
    "    SingleCell(barcode, out_folder, bam_dir, bam_with_RG_dir, vcf_dir,)\n",
    "    for barcode in barcode_recover\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf4a3ad",
   "metadata": {},
   "source": [
    "### Genome alignment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48c9ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ALIGN TO THE REFERENCE GENOME\n",
    "# limit number of cells to preprocess at a time (based on hardware limitations)\n",
    "n_preprocess = 40\n",
    "\n",
    "# create pool of workers and run through all samples\n",
    "preprocess_pool = ThreadPool(processes=n_preprocess)\n",
    "\n",
    "start_time = time.time()  # Start the timer\n",
    "\n",
    "for c in cells:\n",
    "    preprocess_pool.apply_async(SingleCell.align_and_index, args=(c, bwamem2))\n",
    "\n",
    "preprocess_pool.close()\n",
    "preprocess_pool.join()\n",
    "\n",
    "end_time = time.time()  # End the timer\n",
    "execution_time = end_time - start_time  # Calculate execution time\n",
    "\n",
    "# Print execution time\n",
    "print(f\"Execution time: {execution_time:.2f} seconds\")\n",
    "\n",
    "# Save execution time to a text file\n",
    "with open(\"genome_alignment_execution_time.txt\", \"w\") as f:\n",
    "    f.write(f\"Execution time: {execution_time:.2f} seconds\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a6a5683",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add RG information to each bam file\n",
    "# Limit number of BAM files to preprocess at a time (based on hardware limitations)\n",
    "n_preprocess = 80\n",
    "preprocess_pool = ThreadPool(processes=n_preprocess)\n",
    "\n",
    "# Define the paths for input and output directories\n",
    "input_dir = bam_dir\n",
    "#bam_with_RG_dir = os.path.join(out_folder, \"bam_with_RG\")\n",
    "\n",
    "# Ensure the output directory exists\n",
    "#os.makedirs(bam_with_RG_dir, exist_ok=True)\n",
    "\n",
    "# Define the Read Group (RG) information\n",
    "RGID = \"1\"           # Read group ID\n",
    "RGLB = \"lib1\"        # Library\n",
    "RGPL = \"illumina\"    # Platform\n",
    "RGPU = \"unit1\"       # Platform unit\n",
    "RGSM = \"sample1\"     # Sample name\n",
    "\n",
    "# Function to process a single BAM file\n",
    "def process_bam(file_name):\n",
    "    if file_name.endswith('.bam'):\n",
    "        input_bam = os.path.join(input_dir, file_name)\n",
    "        output_bam = os.path.join(bam_with_RG_dir, f\"{os.path.splitext(file_name)[0]}.bam\")\n",
    "        # Construct the GATK command\n",
    "        gatk_command = (\n",
    "            f\"python3 {config['gatk_path']} AddOrReplaceReadGroups \" \\\n",
    "            f\"-I {input_bam} \" \\\n",
    "            f\"-O {output_bam} \" \\\n",
    "            f\"--RGID {RGID} \" \\\n",
    "            f\"--RGLB {RGLB} \" \\\n",
    "            f\"--RGPL {RGPL} \" \\\n",
    "            f\"--RGPU {RGPU} \" \\\n",
    "            f\"--RGSM {RGSM}\"\n",
    "        )\n",
    "        \n",
    "        # Run the GATK command\n",
    "        os.system(gatk_command)\n",
    "        #print(f\"Processed {file_name}\")\n",
    "\n",
    "start_time = time.time()  # Start the timer\n",
    "\n",
    "# List all BAM files in the input directory and process them using ThreadPool\n",
    "bam_files = [f for f in os.listdir(input_dir) if f.endswith('.bam')]\n",
    "preprocess_pool.map(process_bam, bam_files)\n",
    "\n",
    "# Close the ThreadPool\n",
    "preprocess_pool.close()\n",
    "preprocess_pool.join()\n",
    "\n",
    "for file_name in os.listdir(bam_with_RG_dir):\n",
    "    if file_name.endswith('.bam'):\n",
    "        input_bam = os.path.join(bam_with_RG_dir, file_name)\n",
    "        \n",
    "        # Construct the Samtools index command\n",
    "        index_command = f\"{config['samtools_path']} index {input_bam}\"\n",
    "        \n",
    "        # Run the Samtools index command\n",
    "        os.system(index_command)\n",
    "        #print(f\"Indexed {file_name}\")\n",
    "\n",
    "end_time = time.time()  # End the timer\n",
    "execution_time = end_time - start_time  # Calculate execution time\n",
    "\n",
    "# Print execution time\n",
    "print(f\"Execution time: {execution_time:.2f} seconds\")\n",
    "\n",
    "# Save execution time to a text file\n",
    "with open(\"add_RG_execution_time.txt\", \"w\") as f:\n",
    "    f.write(f\"Execution time: {execution_time:.2f} seconds\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "980276c4",
   "metadata": {},
   "source": [
    "### Variant calling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a33e7989",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CALL VARIANTS\n",
    "# limit number of cells to call variants at a time (based on hardware limitations)\n",
    "n_call_variants = 50\n",
    "\n",
    "# create pool of workers and run through all samples\n",
    "call_variants_pool = ThreadPool(processes=n_call_variants)\n",
    "\n",
    "start_time = time.time()  # Start the timer\n",
    "\n",
    "for c in cells:\n",
    "    call_variants_pool.apply_async(SingleCell.call_variants, args=(c, human_fasta_file, interval_file,))\n",
    "\n",
    "call_variants_pool.close()\n",
    "call_variants_pool.join()\n",
    "\n",
    "end_time = time.time()  # End the timer\n",
    "execution_time = end_time - start_time  # Calculate execution time\n",
    "\n",
    "# Print execution time\n",
    "print(f\"Execution time: {execution_time:.2f} seconds\")\n",
    "\n",
    "# Save execution time to a text file\n",
    "with open(\"call_variants_execution_time.txt\", \"w\") as f:\n",
    "    f.write(f\"Execution time: {execution_time:.2f} seconds\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e9704c",
   "metadata": {},
   "source": [
    "# Compute Jaccard indices & UMAP:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a58b391",
   "metadata": {},
   "source": [
    "1. Load truth variants from FTP-GrCH38 for both GM1 and GM2 cell lines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1677b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "truth_variant_file_path = config['truth_GM1_GM2_variant']\n",
    "truth_variant_df = pd.read_csv(truth_variant_file_path, index_col=False)\n",
    "truth_variant_df['Variant_Location'] = truth_variant_df['CHROM'] + \":\" + truth_variant_df['POS'].astype(str)\n",
    "truth_variant_df\n",
    "truth_variant_df = truth_variant_df.pivot_table(index='CELL_LINES', columns='Variant_Location', values='TRUTH_CALL', aggfunc='first')\n",
    "truth_variant_df = truth_variant_df.fillna(0)\n",
    "truth_variant_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02bcfbae",
   "metadata": {},
   "source": [
    "2. File format (vcf to csv):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a17cf31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_vcf_to_csv(vcf_dir, vcf_csv_dir):\n",
    "    \"\"\"\n",
    "    Convert all .g.vcf files in vcf_dir to CSV format and save them in vcf_csv_dir.\n",
    "    \n",
    "    :param vcf_dir: Directory containing VCF files\n",
    "    :param vcf_csv_dir: Directory to save CSV files\n",
    "    \"\"\"\n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(vcf_csv_dir, exist_ok=True)\n",
    "    \n",
    "    # Loop through all .g.vcf files in the input directory\n",
    "    for filename in os.listdir(vcf_dir):\n",
    "        if filename.endswith(\".g.vcf\"):  # Process only VCF files\n",
    "            vcf_file = os.path.join(vcf_dir, filename)\n",
    "            csv_file = os.path.join(vcf_csv_dir, filename.replace(\".g.vcf\", \".csv\"))\n",
    "            \n",
    "            # Open VCF and CSV files\n",
    "            with open(vcf_file, 'r') as vcf, open(csv_file, 'w', newline='') as csv_out:\n",
    "                vcf_reader = vcf.readlines()\n",
    "                csv_writer = csv.writer(csv_out)\n",
    "                \n",
    "                # Write the header row for the CSV\n",
    "                csv_writer.writerow(['CHROM', 'POS', 'ID', 'REF', 'ALT', 'QUAL', 'FILTER', 'GT', 'DP', 'GQ', 'AD', 'PL', 'SB'])\n",
    "                \n",
    "                # Loop through each line in the VCF file\n",
    "                for line in vcf_reader:\n",
    "                    # Skip header lines starting with '#'\n",
    "                    if line.startswith('#'):\n",
    "                        continue\n",
    "                    \n",
    "                    # Split the VCF line into columns\n",
    "                    columns = line.strip().split('\\t')\n",
    "                    \n",
    "                    # Extract the relevant fields\n",
    "                    chrom = columns[0]       # CHROM\n",
    "                    pos = columns[1]         # POS\n",
    "                    variant_id = columns[2]  # ID\n",
    "                    ref = columns[3]         # REF\n",
    "                    alt = columns[4]         # ALT\n",
    "                    qual = columns[5]        # QUAL\n",
    "                    filter_field = columns[6]  # FILTER\n",
    "                    format_field = columns[8]  # FORMAT (contains GT, DP, GQ, etc.)\n",
    "                    sample_data = columns[9]   # Sample data (e.g., for sample1)\n",
    "                    \n",
    "                    # Split the FORMAT and sample data to extract all fields\n",
    "                    format_values = format_field.split(':')\n",
    "                    sample_values = sample_data.split(':')\n",
    "                    \n",
    "                    # Create a dictionary to map the format fields to sample data\n",
    "                    sample_info = dict(zip(format_values, sample_values))\n",
    "                    \n",
    "                    # Extract values for GT, DP, GQ, AD, PL, and SB\n",
    "                    gt = sample_info.get('GT', 'NA')\n",
    "                    dp = sample_info.get('DP', 'NA')\n",
    "                    #total_dp = sum(map(int, dp.split(','))) if isinstance(dp, str) and ',' in dp else int(dp) if dp.isdigit() else 'NA'\n",
    "                    gq = sample_info.get('GQ', 'NA')\n",
    "                    ad = sample_info.get('AD', 'NA')\n",
    "                    pl = sample_info.get('PL', 'NA')\n",
    "                    sb = sample_info.get('SB', 'NA')\n",
    "                    \n",
    "                    # Write the row to the CSV\n",
    "                    csv_writer.writerow([chrom, pos, variant_id, ref, alt, qual, filter_field, gt, dp, gq, ad, pl, sb])\n",
    "            \n",
    "            #print(f\"Converted: {vcf_file} -> {csv_file}\")\n",
    "    \n",
    "    print(\"Batch VCF to CSV conversion complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fefcc57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def genotype_compiling(folder_path, min_read_filters, output_file_path):\n",
    "    # Create an empty master dataframe\n",
    "    master_df = pd.DataFrame()\n",
    "    \n",
    "    # Loop through all CSV files in the specified folder\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith('.csv'):\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            df = pd.read_csv(file_path)\n",
    "\n",
    "            # Replace \"GT\" values with 3 if \"DP\" is less than min_read_filters\n",
    "            df.loc[df['DP'] < min_read_filters, 'GT'] = '3'\n",
    "\n",
    "            # Create \"Variant_Location\" column\n",
    "            df['Variant_Location'] = df['CHROM'] + \":\" + df['POS'].astype(str)\n",
    "\n",
    "            # Map \"GT\" values\n",
    "            gt_mapping = {\n",
    "                '0/0': 0,\n",
    "                '0/1': 1,\n",
    "                '1/1': 2,\n",
    "            }\n",
    "            df['GT'] = df['GT'].map(gt_mapping).fillna(3).astype(int)\n",
    "\n",
    "            # Create a new dataframe for the current file\n",
    "            current_df = df[['Variant_Location', 'GT']].set_index('Variant_Location')\n",
    "            current_df = current_df.rename(columns={'GT': filename})\n",
    "\n",
    "            # Merge the current dataframe into the master dataframe\n",
    "            master_df = master_df.join(current_df, how='outer')\n",
    "\n",
    "    # Transpose the master dataframe\n",
    "    transposed_df = master_df.T\n",
    "\n",
    "    # Save the transposed dataframe to a CSV file in the output folder\n",
    "    transposed_df.to_csv(output_file_path)\n",
    "\n",
    "    return transposed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c87ba5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_vcf_to_csv(vcf_dir, vcf_csv_dir)\n",
    "min_read_filter = config['minimum_reads_variant_filter']\n",
    "genotype_compiling(vcf_csv_dir, min_read_filter, out_folder + '/genotype_compiling_threshold_' + str(min_read_filter) + \".csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de4ad253",
   "metadata": {},
   "source": [
    "3. Compute Jaccard index and UMAP + knn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f115e1aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_index_with_all(df, row_idx):\n",
    "    \"\"\"\n",
    "    Calculate the Jaccard Index between one row and all rows in a dataframe.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The dataframe.\n",
    "        row_idx (int): Index of the row to compare against all rows.\n",
    "\n",
    "    Returns:\n",
    "        pd.Series: Jaccard indices for each row in the dataframe.\n",
    "    \"\"\"\n",
    "    # Select the target row\n",
    "    target_row = df.loc[row_idx]\n",
    "\n",
    "    # Calculate Jaccard Index for each row\n",
    "    def calculate_jaccard(row):\n",
    "        # Intersection: Values that match between the two rows\n",
    "        intersection = (target_row == row).sum()\n",
    "        # Union: Total length minus the intersection\n",
    "        union = 2*len(row) - intersection\n",
    "        # Jaccard Index (skip division by zero check for now)\n",
    "        return intersection / union\n",
    "\n",
    "    # Apply the Jaccard Index calculation to all rows\n",
    "    jaccard_indices = df.apply(calculate_jaccard, axis=1)\n",
    "\n",
    "    return jaccard_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da8ea123",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_barnyard(row, jaccard_threshold = config['jaccard_threshold']):\n",
    "    if row[\"Jaccard_GM12878\"] > jaccard_threshold and row[\"Jaccard_GM24385\"] < jaccard_threshold:\n",
    "        return \"GM12878\"\n",
    "    elif row[\"Jaccard_GM24385\"] > jaccard_threshold and row[\"Jaccard_GM12878\"] < jaccard_threshold:\n",
    "        return \"GM24385\"\n",
    "    elif row[\"Jaccard_GM24385\"] > jaccard_threshold and row[\"Jaccard_GM12878\"] > jaccard_threshold:\n",
    "        return \"Mixed\"\n",
    "    else:\n",
    "        return \"Junk\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "277424ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "genotype_compiling_df = pd.read_csv(out_folder + \"/genotype_compiling_threshold_\" + str(min_read_filter) + \".csv\", index_col='Unnamed: 0')\n",
    "genotype_compiling_df_combine = pd.concat([truth_variant_df, genotype_compiling_df])\n",
    "cell_line_1_jaccard = jaccard_index_with_all(genotype_compiling_df_combine, 'GM12878')\n",
    "cell_line_2_jaccard = jaccard_index_with_all(genotype_compiling_df_combine, 'GM24385')\n",
    "jaccard_metrics_df = pd.DataFrame({'Jaccard_GM12878': cell_line_1_jaccard, 'Jaccard_GM24385': cell_line_2_jaccard})\n",
    "\n",
    "#umap\n",
    "umap_model = umap.UMAP(n_neighbors=config['umap_n_neighbor'], metric='hamming')\n",
    "umap_results = umap_model.fit_transform(genotype_compiling_df_combine)\n",
    "umap_results_df = pd.DataFrame(umap_results, columns=['UMAP1', 'UMAP2'], index=genotype_compiling_df_combine.index)\n",
    "\n",
    "#knn\n",
    "kmeans = KMeans(n_clusters=3, random_state=42).fit(umap_results)\n",
    "labels = kmeans.labels_\n",
    "umap_results_df['Cluster'] = labels\n",
    "\n",
    "#plot umap\n",
    "plt.figure(figsize=(7, 7))\n",
    "scatter = sns.scatterplot(data=umap_results_df, x='UMAP1', y='UMAP2', hue='Cluster')\n",
    "for index in ['GM12878', 'GM24385']:\n",
    "    if index in umap_results_df.index:\n",
    "        point = umap_results_df.loc[index]\n",
    "        scatter.scatter(point['UMAP1'], point['UMAP2'], color='red', s=100)  # Mark the points in red\n",
    "        scatter.text(point['UMAP1'], point['UMAP2'], index, color='red', fontsize=12, ha='right')\n",
    "plt.savefig(out_folder + \"/UMAP_clustering_min_\" + str(min_read_filter) + \"_read_filter.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# UMAP cell annotation\n",
    "cluster_to_cell = {\n",
    "    umap_results_df.loc[\"GM12878\", \"Cluster\"]: \"GM12878\",\n",
    "    umap_results_df.loc[\"GM24385\", \"Cluster\"]: \"GM24385\"\n",
    "}\n",
    "\n",
    "umap_results_df[\"UMAP_Cell_Annotation\"] = umap_results_df[\"Cluster\"].map(lambda x: cluster_to_cell.get(x, \"Non-determined\"))\n",
    "variant_metric_df = jaccard_metrics_df.merge(umap_results_df, left_index=True, right_index=True, how=\"inner\")\n",
    "variant_metric_df.to_csv(out_folder + \"/Variant_call_related_min_\" + str(min_read_filter) + \"_read_filter.csv\")\n",
    "\n",
    "hue_array = ['values'] * variant_metric_df.shape[0]\n",
    "jitter_strength_x = 0.02  \n",
    "jitter_strength_y = 0.02\n",
    "variant_metric_df[\"Jaccard_GM12878_jittered\"] = variant_metric_df[\"Jaccard_GM12878\"] + np.random.uniform(-jitter_strength_x, jitter_strength_x, size=len(variant_metric_df))\n",
    "variant_metric_df[\"Jaccard_GM24385_jittered\"] = variant_metric_df[\"Jaccard_GM24385\"] + np.random.uniform(-jitter_strength_x, jitter_strength_x, size=len(variant_metric_df))\n",
    "variant_metric_df[\"Barnyard_cell_annotation\"] = variant_metric_df.apply(classify_barnyard, axis=1)\n",
    "\n",
    "plt.figure(figsize=(7, 7))\n",
    "g = sns.jointplot(data=variant_metric_df, x='Jaccard_GM12878_jittered', y='Jaccard_GM24385_jittered', alpha=0.03, hue= hue_array)\n",
    "g.fig.suptitle('Barnyard plot')\n",
    "plt.xlim(-0.1, 1.05)\n",
    "plt.ylim(-0.1, 1.05)\n",
    "plt.legend()\n",
    "plt.savefig(out_folder + \"/Barnyard_plot_min_\" + str(min_read_filter) + \"_read_filter.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.figure(figsize=(7, 7))\n",
    "g = sns.jointplot(data=variant_metric_df, x='Jaccard_GM12878_jittered', y='Jaccard_GM24385_jittered', alpha=0.1, hue= 'UMAP_Cell_Annotation')\n",
    "g.fig.suptitle('Barnyard plot overlaid with UMAP')\n",
    "plt.xlim(-0.1, 1.05)\n",
    "plt.ylim(-0.1, 1.05)\n",
    "plt.legend()\n",
    "plt.savefig(out_folder + \"/Barnyard_plot_UMAP_overlay_min_\" + str(min_read_filter) + \"_read_filter.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "total_barcodes = variant_metric_df.shape[0]\n",
    "GM12878_barnyard = (variant_metric_df[\"Barnyard_cell_annotation\"] == \"GM12878\").sum()\n",
    "GM24385_barnyard = (variant_metric_df[\"Barnyard_cell_annotation\"] == \"GM24385\").sum()\n",
    "Mixed_barnyard = (variant_metric_df[\"Barnyard_cell_annotation\"] == \"Mixed\").sum()\n",
    "Junk_barnyard = (variant_metric_df[\"Barnyard_cell_annotation\"] == \"Junk\").sum()\n",
    "GM12878_umap = (variant_metric_df[\"UMAP_Cell_Annotation\"] == \"GM12878\").sum()\n",
    "GM24385_umap = (variant_metric_df[\"UMAP_Cell_Annotation\"] == \"GM24385\").sum()\n",
    "Non_determined_umap = (variant_metric_df[\"UMAP_Cell_Annotation\"] == \"Non-determined\").sum()\n",
    "\n",
    "cell_annotation_result = {\n",
    "'Metric': ['GM12878_barnyard', 'GM24385_barnyard', 'Mixed_barnyard', 'Junk_barnyard', 'GM12878_umap', 'GM24385_umap', 'Non_determined_umap'],\n",
    "'Raw_counts': [GM12878_barnyard, GM24385_barnyard, Mixed_barnyard, Junk_barnyard, GM12878_umap, GM24385_umap, Non_determined_umap]\n",
    "}\n",
    "cell_annotation_result = pd.DataFrame(cell_annotation_result)\n",
    "cell_annotation_result['Percentages'] = cell_annotation_result['Raw_counts']/total_barcodes * 100\n",
    "cell_annotation_result.to_csv(out_folder + \"/Cell_annotation_result_min_\" + str(min_read_filter) + \"_read_filter.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
