{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a233039e",
   "metadata": {},
   "source": [
    "## Install packages & dependecies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d98630db",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "%%bash\n",
    "#Cyrille's github\n",
    "git clone https://github.com/cdelley/scyBCB\n",
    "pip install git+https://github.com/cdelley/scyBCB\n",
    "\n",
    "#Install python dependencies\n",
    "pip install watermark\n",
    "pip install anndata\n",
    "pip install \"numpy<2\"\n",
    "pip install xopen\n",
    "pip install PyQt5\n",
    "\n",
    "#Install GATK 4.1.3.0 because Java on cluster is 11 and only compatible with GATK 4.1.x.x.\n",
    "!unzip /home/caleb/scDNA_analysis/gatk-4.1.3.0.zip\n",
    "\n",
    "#Install bwa-mem2 (Linux)\n",
    "!curl -L https://github.com/bwa-mem2/bwa-mem2/releases/download/v2.2.1/bwa-mem2-2.2.1_x64-linux.tar.bz2 \\\n",
    "  | tar jxf -\n",
    "  \n",
    "#Install yaml\n",
    "!pip install pyyaml\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a29102",
   "metadata": {},
   "source": [
    "## Import libraries & Tool setup:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "219cadd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the github repo\n",
    "import scyBCB.CyGeno as dabm\n",
    "import scyBCB.CyBCB as bcb\n",
    "\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import subprocess\n",
    "import gzip\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import subprocess\n",
    "import yaml\n",
    "\n",
    "#matplotlib.use('Qt5Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "matplotlib.use('Agg')\n",
    "%matplotlib inline\n",
    "\n",
    "%load_ext watermark\n",
    "%watermark -v -iv\n",
    "\n",
    "from collections import Counter\n",
    "from multiprocessing.pool import ThreadPool\n",
    "from multiprocessing import Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b89597c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load YAML configuration file\n",
    "with open('config_example.yaml', 'r') as file:\n",
    "    config = yaml.safe_load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98bb9ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105b81a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!chmod +x {config['demuxbyname_path']}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "403fc9d6",
   "metadata": {},
   "source": [
    "## File/Folder directories (change here):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e33c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Barcode whitelist help to calculate cell barcode base offset (1-3 nucleotide btw BC1 and BC2) if beads are synthesized in split-pool approaches.\n",
    "barcodes = [config['barcode_whitelist_1'],\n",
    "            config['barcode_whitelist_2']]\n",
    "bc_dicts = []\n",
    "for bc_json in barcodes:\n",
    "    with open(bc_json, 'r') as fin:\n",
    "        bc_dicts.append(json.load(fin))\n",
    "        \n",
    "# Forward and reverse primer .csv files of the targeted DNA panels (AML or ML):\n",
    "primers = pd.read_csv(config['primer_csv'])\n",
    "primer_dict_fw = {s[:19]:n+'_fw' for (s,n) in zip(list(primers['fwd_seq']), list(primers['AmpID']))}\n",
    "primer_dict_rev = {s[:19]:n+'_rev' for (s,n) in zip(list(primers['rev_seq']), list(primers['AmpID']))}\n",
    "\n",
    "# Directory to Read 1 and Read 2 files\n",
    "f1 = config['read_1_file']\n",
    "f2 = config['read_2_file']\n",
    "\n",
    "# Output folder (to dump processed sequencing file in)\n",
    "out_folder = config['fastq_out_folder']\n",
    "os.makedirs(out_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "219e4ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Barcode template:\n",
    "# MissionBio beads\n",
    "MB_seq_pos_dict = {\n",
    "    'bc'  : [(0,0,9), (0,23,32)], # two barcodes\n",
    "    'feature' : [(0,47,66), (1,0,19)],        # features, in this case, are the primer positions for stats, normally this is to indicate antibody barcode tags\n",
    "    'seq': [(0,66,151), (1,0,151)],           # sequence positions including the primer region. These are copied to the fastq output\n",
    "}\n",
    "\n",
    "# Lab-made beads:\n",
    "LM_seq_pos_dict = {\n",
    "    'bc'  : [(0,0,11), (0,12,20), (0,24,32)], # three barcodes\n",
    "    'feature' : [(0,51,70), (1,0,19)],        \n",
    "    'seq': [(0,51,151), (1,0,151)],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ebc82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bwa-mem2 index location\n",
    "bwamem2 = config['bwamem2']\n",
    "# folder directories for bam and vcf file\n",
    "bam_dir = f\"{out_folder}/bam/\"\n",
    "bam_with_RG_dir = f\"{out_folder}/bam_with_RG/\"\n",
    "vcf_dir = f\"{out_folder}/vcf/\"\n",
    "vcf_csv_dir = f\"{out_folder}/vcf_csv/\"\n",
    "os.makedirs(bam_dir, exist_ok=True)\n",
    "os.makedirs(bam_with_RG_dir, exist_ok=True)\n",
    "os.makedirs(vcf_dir, exist_ok=True)\n",
    "\n",
    "# reference genome file and primer location for variant callling\n",
    "human_fasta_file = config['human_fastq_file']\n",
    "interval_file = config['interval_file'] #only look at amplicons that contain GM1 & GM2 variants"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ece69ee",
   "metadata": {},
   "source": [
    "## Filter reads based on cell barcode & fwd/rvs primer alignment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b346d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "exp = dabm.sequences.write_good_reads(\n",
    "    file1=f1,\n",
    "    file2=f2, \n",
    "    bc_correction_dic=bc_dicts,\n",
    "    primer_dic=[primer_dict_fw, primer_dict_rev],\n",
    "    out_path=out_folder,\n",
    "    seq_positions=MB_seq_pos_dict,\n",
    "    verbose=True, \n",
    "    #down_sample=10**3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18674709",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recover_barcode(barcode_count_dict, low_count_filter=200, batch_label=''):\n",
    "    \"\"\"\n",
    "    This functions filter barcodes based on the total number of reads per barcode.\n",
    "    The default minimum total of reads required is 200.\n",
    "    barcode_count_dict: dictionaries from write_good_reads function where each key is the barcode, each value is the total read counts\n",
    "    low_count_filter: minimum of reads per barcode to be selected for downstream processing\n",
    "    \"\"\"\n",
    "    \n",
    "    # count reads per barcode\n",
    "    reads = np.array(list(barcode_count_dict.values()))\n",
    "    _thresh_idx = reads >= low_count_filter\n",
    "\n",
    "    bcs = np.array(list(barcode_count_dict.keys()))[_thresh_idx]  \n",
    "    cell_barcodes = np.sort(bcs)\n",
    "\n",
    "    barcode_count_touples = list(barcode_count_dict.items())\n",
    "    return cell_barcodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c0582ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "barcode_recover = recover_barcode(exp.bc_groups)\n",
    "barcode_string = barcode_recover.astype(str)\n",
    "np.savetxt(out_folder + \"/filtered_barcode_string.txt\", barcode_string, fmt='%s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136174c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# demultiplex the R1 and R2 into multiple fastq.gz file for each barcode\n",
    "with open(f'{out_folder}/barcode_recover', 'w') as fout:\n",
    "    for i in barcode_recover:\n",
    "        fout.write(str(i)+',')\n",
    "\n",
    "# Change here for path to bbmap\n",
    "_pp1 = f\"{config['demuxbyname_path']} -Xmx60g \"\n",
    "_pp2 = f\"in={out_folder}/R1.fastq.gz in2={out_folder}/R2.fastq.gz out={out_folder}/demuxed_all/cell_%.fastq.gz  \"\n",
    "_pp3 = f\"delimiter=_ suffixmode=t names={out_folder}/filtered_barcode_string.txt\"\n",
    "\n",
    "cmd = _pp1+_pp2+_pp3\n",
    "subprocess.run(cmd, shell = True, executable=\"/bin/bash\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c36136d",
   "metadata": {},
   "source": [
    "## Target amplicon read counts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "019eb7c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fastq_to_txt(input_folder, output_folder):\n",
    "    \"\"\"\n",
    "    This function is to extract R2 from .fastq.gz files into txt files\n",
    "    We can cut down this function somehow in the future to increase speed of the pipeline\n",
    "    input_folder: folder containing .fastq.gz files\n",
    "    output_folder: folder containing .txt files (that are converted from .fastq.gz files)\n",
    "    \"\"\"\n",
    "    # Ensure output folder exists\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    # Regular expression to capture the sequence after \"-R2-\" up to the underscore\n",
    "    pattern = re.compile(r\"@.*-R2-([A-Z]+)\")\n",
    "\n",
    "    # Loop through each FASTQ file in the input folder\n",
    "    for filename in os.listdir(input_folder):\n",
    "        if filename.endswith(\".fastq.gz\"):\n",
    "            input_path = os.path.join(input_folder, filename)\n",
    "            output_filename = filename.replace(\".fastq.gz\", \"_R2_sequences.txt\")\n",
    "            output_path = os.path.join(output_folder, output_filename)\n",
    "\n",
    "            # Open the output file to write extracted sequences\n",
    "            with open(output_path, \"w\") as output_file:\n",
    "                # Open and read the compressed FASTQ file\n",
    "                with gzip.open(input_path, \"rt\") as fastq_file:\n",
    "                    for line in fastq_file:\n",
    "                        # Check if the line is a header with \"-R2-\"\n",
    "                        match = pattern.match(line)\n",
    "                        if match:\n",
    "                            # Write the extracted sequence to the output file\n",
    "                            sequence_after_R2 = match.group(1)\n",
    "                            output_file.write(sequence_after_R2 + \"\\n\")\n",
    "\n",
    "            print(f\"Processed {filename} and saved to {output_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee0075b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fastq_to_txt(out_folder+\"/demuxed_all\", out_folder+\"/R2_txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b0657e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start counting number of reads align to each amplicon per R2 values\n",
    "R2_txt_folder = out_folder + \"/R2_txt\"\n",
    "# Initialize an empty DataFrame to store the counts\n",
    "count_table = pd.DataFrame(columns=primer_dict_rev.values())\n",
    "\n",
    "# Loop through each .txt file in the output directory\n",
    "for filename in os.listdir(R2_txt_folder):\n",
    "    if filename.endswith(\"_R2_sequences.txt\"):\n",
    "        file_path = os.path.join(R2_txt_folder, filename)\n",
    "\n",
    "        # Count occurrences where a sequence aligns with any target primer\n",
    "        with open(file_path, \"r\") as f:\n",
    "            sequences = f.read().splitlines()\n",
    "            file_counts = {target_name: 0 for target_name in primer_dict_rev.values()}\n",
    "\n",
    "            for sequence in sequences:\n",
    "                for target_seq, target_name in primer_dict_rev.items():\n",
    "                    # Check if target sequence is a substring of the read sequence\n",
    "                    if sequence in target_seq:\n",
    "                        file_counts[target_name] += 1\n",
    "\n",
    "        # Add the counts as a new row in the DataFrame, with the file name as the index\n",
    "        count_table.loc[filename] = file_counts\n",
    "\n",
    "# Display the final table\n",
    "count_table.index.name = \"File\"\n",
    "count_table.index = count_table.index.str.split('_R2').str[0]\n",
    "count_table.to_csv(out_folder + \"/target_amplicon_counts.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76ee10fd",
   "metadata": {},
   "source": [
    "## Read-count related metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "262159af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gini_coefficient(read_counts):\n",
    "    \"\"\"    \n",
    "    This function calculates the Gini coefficient per barcode\n",
    "    \n",
    "    Parameters:\n",
    "    read_counts (list or numpy array): A list or array of read counts per barcode\n",
    "    \n",
    "    Returns:\n",
    "    float: The Shannon entropy per barcode.\n",
    "    \"\"\"\n",
    "    read_counts = sorted(read_counts)\n",
    "    n = len(read_counts)\n",
    "    total_read_counts = sum(read_counts)\n",
    "    if total_read_counts == 0:\n",
    "        return 0\n",
    "    gini_sum = sum((2 * (i+1) - n - 1) * read_counts[i] for i in range(n))\n",
    "    gini = gini_sum / (n * total_read_counts)\n",
    "    return gini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da702bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shannon_entropy(read_counts):\n",
    "    \"\"\"\n",
    "    Calculate Shannon Entropy for a set of read counts, handling zero read counts appropriately.\n",
    "    \n",
    "    Parameters:\n",
    "    read_counts (list or numpy array): A list or array of read counts per barcode\n",
    "    \n",
    "    Returns:\n",
    "    float: The Shannon entropy per barcode.\n",
    "    \"\"\"\n",
    "    # Convert read counts to a numpy array for easier handling\n",
    "    read_counts = np.array(read_counts)\n",
    "    \n",
    "    # Normalize the read counts to obtain probabilities (p_i)\n",
    "    total_reads = np.sum(read_counts)\n",
    "    \n",
    "    probabilities = read_counts / total_reads\n",
    "    \n",
    "    # Calculate Shannon entropy, ignoring zero probabilities\n",
    "    entropy = 0.0\n",
    "    for p in probabilities:\n",
    "        if p > 0:  # Ignore terms where p_i is zero\n",
    "            entropy -= p * np.log2(p)\n",
    "    \n",
    "    return entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f51fcbe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fraction_amplicons_above_threshold(read_counts, threshold):\n",
    "    \"\"\"\n",
    "    This function calculates the fraction of amplicons with read counts greater than or equal to the threshold.\n",
    "    \n",
    "    Parameters:\n",
    "    read_counts (list): A list of read counts for each amplicon.\n",
    "    threshold (int or float): The threshold value for filtering read counts.\n",
    "    \n",
    "    Returns:\n",
    "    float: Fraction of amplicons with read counts >= threshold.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Total number of amplicons\n",
    "    total_amplicons = len(read_counts)\n",
    "    \n",
    "    # Avoid division by zero if there are no amplicons\n",
    "    if total_amplicons == 0:\n",
    "        return 0\n",
    "    \n",
    "    # Count the number of amplicons with reads >= threshold\n",
    "    amplicons_above_threshold = sum(1 for count in read_counts if count >= threshold)\n",
    "    \n",
    "    # Calculate the fraction\n",
    "    fraction = amplicons_above_threshold / total_amplicons\n",
    "    \n",
    "    return fraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87618ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate metrics and compile into the target_amplicon_counts.csv\n",
    "total_reads = count_table.sum(axis=1)\n",
    "Gini = count_table.apply(lambda row: gini_coefficient(row), axis=1)\n",
    "evenness = 1 - Gini\n",
    "entropy = count_table.apply(lambda row: shannon_entropy(row), axis=1)\n",
    "x1 = count_table.apply(lambda row: fraction_amplicons_above_threshold(row.tolist(), 1), axis=1)\n",
    "x10 = count_table.apply(lambda row: fraction_amplicons_above_threshold(row.tolist(), 10), axis=1)\n",
    "x20 = count_table.apply(lambda row: fraction_amplicons_above_threshold(row.tolist(), 20), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61930700",
   "metadata": {},
   "outputs": [],
   "source": [
    "read_count_quality_metrics = pd.DataFrame({'Total_reads': total_reads,\n",
    "                                          'Evenness': evenness,\n",
    "                                          'Entropy': entropy,\n",
    "                                          'x1': x1,\n",
    "                                          'x10': x10,\n",
    "                                          'x20': x20})\n",
    "read_count_quality_metrics.to_csv(out_folder + \"/read_count_quality_metrics.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0715d588",
   "metadata": {},
   "source": [
    "## Variant calling & Cell line analysis:\n",
    "This code is optimized towards 2 cell lines analysis **ONLY**\n",
    "If you need different cell lines than GM1 and GM2, please contact Caleb @ caleb_thinh_tong@berkeley.edu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e97100",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SingleCell(object):\n",
    "    # class for storing metadata for each single cell file\n",
    "\n",
    "    def __init__(self, cell_barcode, out_folder, bam_dir, bam_with_RG_dir, vcf_dir):\n",
    "        # initialize object by generating filenames\n",
    "\n",
    "        self.cell_barcode = cell_barcode                    # cell barcode\n",
    "        self.fastq = f\"{out_folder}demuxed_all/cell_{cell_barcode}.fastq.gz\"     # fastq file\n",
    "\n",
    "        self.bam = bam_dir + \"cell_\" + cell_barcode + '.bam'          # bam file\n",
    "        self.bai = bam_dir + \"cell_\" + cell_barcode + '.bai'          # bam file index\n",
    "        self.bam_with_RG = bam_with_RG_dir + \"cell_\" + cell_barcode + '.bam'          # bam file\n",
    "        self.bai_with_RG = bam_with_RG_dir + \"cell_\" + cell_barcode + '.bai'          # bam file index\n",
    "        self.vcf = vcf_dir + \"cell_\" + cell_barcode + '.g.vcf'        # gvcf file\n",
    "\n",
    "        self.valid = False      # marker for valid cells\n",
    "        self.alignments = {}    # alignment counts for each interval\n",
    "\n",
    "    def align_and_index(self, ref_idx, align_cmd = 'bwa-mem2'):\n",
    "\n",
    "        # align the panel to the bowtie2 human index and generate sorted bam file\n",
    "        if align_cmd == 'bwa-mem2':\n",
    "            align_cmd = f\"{config['bwamem2_path']} mem -t1\" \\\n",
    "                        f' {ref_idx:s} {self.fastq:s}'\n",
    "        elif align_cmd == 'minimap2':\n",
    "            pass\n",
    "        elif align_cmd == 'bowtie2':\n",
    "            align_cmd = f'bowtie2 -x {ref_idx} --mm --interleaved {self.fastq} --rg-id {self.cell_barcode} ' \\\n",
    "                        f'--rg SM:{self.cell_barcode} --rg PL:ILLUMINA --rg CN:UCSF --quiet'\n",
    "        else:\n",
    "            align_cmd = align_cmd\n",
    "\n",
    "        # filter and sort the output\n",
    "        align_cmd = align_cmd + f\"| {config['samtools_path']} view -b -q 3 -F 4 -F 0X0100 | {config['samtools_path']} sort -o {self.bam:s}\"\n",
    "        \n",
    "        #print(align_cmd)\n",
    "        subprocess.call(align_cmd, shell=True)\n",
    "\n",
    "        # index all bam files using samtools\n",
    "        index_cmd = f\"{config['samtools_path']} index {self.bam} {self.bai}\"\n",
    "        subprocess.call(index_cmd, shell=True)\n",
    "\n",
    "    def call_variants(self, fasta, interval_file):\n",
    "        # call variants using gatk\n",
    "\n",
    "        variants_cmd = f\"python3 {config['gatk_path']} HaplotypeCaller \" \\\n",
    "                       f'-R %s -I %s -O %s -L %s ' \\\n",
    "                       f'--verbosity ERROR ' \\\n",
    "                       f'--native-pair-hmm-threads 1 ' \\\n",
    "                       f'--standard-min-confidence-threshold-for-calling 0 ' \\\n",
    "                       f'--emit-ref-confidence GVCF ' \\\n",
    "                       f'--max-reads-per-alignment-start 0 ' \\\n",
    "                       f'--max-alternate-alleles 2 ' \\\n",
    "                       f'--minimum-mapping-quality 0' \\\n",
    "                       % (fasta,\n",
    "                          self.bam_with_RG,\n",
    "                          self.vcf,\n",
    "                          interval_file)\n",
    "\n",
    "        #print(variants_cmd)\n",
    "        #process = subprocess.Popen(variants_cmd, shell=True)\n",
    "        subprocess.call(variants_cmd, shell=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "470c29b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = out_folder + \"/filtered_barcode_string.txt\"\n",
    "with open(file_path, 'r') as file:\n",
    "    barcode_recover = file.readlines()\n",
    "barcode_recover = [line.strip() for line in barcode_recover]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7376a82f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cells = [\n",
    "    SingleCell(barcode, out_folder, bam_dir, bam_with_RG_dir, vcf_dir,)\n",
    "    for barcode in barcode_recover\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf4a3ad",
   "metadata": {},
   "source": [
    "### Genome alignment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48c9ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ALIGN TO THE REFERENCE GENOME\n",
    "# limit number of cells to preprocess at a time (based on hardware limitations)\n",
    "n_preprocess = 80\n",
    "\n",
    "# create pool of workers and run through all samples\n",
    "preprocess_pool = ThreadPool(processes=n_preprocess)\n",
    "\n",
    "for c in cells:\n",
    "    preprocess_pool.apply_async(SingleCell.align_and_index, args=(c, bwamem2))\n",
    "\n",
    "preprocess_pool.close()\n",
    "preprocess_pool.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a6a5683",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add RG information to each bam file\n",
    "# Limit number of BAM files to preprocess at a time (based on hardware limitations)\n",
    "n_preprocess = 80\n",
    "preprocess_pool = ThreadPool(processes=n_preprocess)\n",
    "\n",
    "# Define the paths for input and output directories\n",
    "input_dir = bam_dir\n",
    "#bam_with_RG_dir = os.path.join(out_folder, \"bam_with_RG\")\n",
    "\n",
    "# Ensure the output directory exists\n",
    "#os.makedirs(bam_with_RG_dir, exist_ok=True)\n",
    "\n",
    "# Define the Read Group (RG) information\n",
    "RGID = \"1\"           # Read group ID\n",
    "RGLB = \"lib1\"        # Library\n",
    "RGPL = \"illumina\"    # Platform\n",
    "RGPU = \"unit1\"       # Platform unit\n",
    "RGSM = \"sample1\"     # Sample name\n",
    "\n",
    "# Function to process a single BAM file\n",
    "def process_bam(file_name):\n",
    "    if file_name.endswith('.bam'):\n",
    "        input_bam = os.path.join(input_dir, file_name)\n",
    "        output_bam = os.path.join(bam_with_RG_dir, f\"{os.path.splitext(file_name)[0]}.bam\")\n",
    "        # Construct the GATK command\n",
    "        gatk_command = (\n",
    "            f\"python3 {config['gatk_path']} AddOrReplaceReadGroups \" \\\n",
    "            f\"-I {input_bam} \" \\\n",
    "            f\"-O {output_bam} \" \\\n",
    "            f\"--RGID {RGID} \" \\\n",
    "            f\"--RGLB {RGLB} \" \\\n",
    "            f\"--RGPL {RGPL} \" \\\n",
    "            f\"--RGPU {RGPU} \" \\\n",
    "            f\"--RGSM {RGSM}\"\n",
    "        )\n",
    "        \n",
    "        # Run the GATK command\n",
    "        os.system(gatk_command)\n",
    "        #print(f\"Processed {file_name}\")\n",
    "\n",
    "# List all BAM files in the input directory and process them using ThreadPool\n",
    "bam_files = [f for f in os.listdir(input_dir) if f.endswith('.bam')]\n",
    "preprocess_pool.map(process_bam, bam_files)\n",
    "\n",
    "# Close the ThreadPool\n",
    "preprocess_pool.close()\n",
    "preprocess_pool.join()\n",
    "\n",
    "for file_name in os.listdir(bam_with_RG_dir):\n",
    "    if file_name.endswith('.bam'):\n",
    "        input_bam = os.path.join(bam_with_RG_dir, file_name)\n",
    "        \n",
    "        # Construct the Samtools index command\n",
    "        index_command = f\"{config['samtools_path']} index {input_bam}\"\n",
    "        \n",
    "        # Run the Samtools index command\n",
    "        os.system(index_command)\n",
    "        #print(f\"Indexed {file_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "980276c4",
   "metadata": {},
   "source": [
    "### Variant calling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a33e7989",
   "metadata": {},
   "outputs": [],
   "source": [
    "# limit number of cells to call variants at a time (based on hardware limitations)\n",
    "n_call_variants = 50\n",
    "\n",
    "# create pool of workers and run through all samples\n",
    "call_variants_pool = ThreadPool(processes=n_call_variants)\n",
    "\n",
    "for c in cells:\n",
    "    call_variants_pool.apply_async(SingleCell.call_variants, args=(c, human_fasta_file, interval_file,))\n",
    "\n",
    "call_variants_pool.close()\n",
    "call_variants_pool.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c723fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert vcf file to csv file for downstream processing\n",
    "def process_vcf_file(vcf_path, output_folder):\n",
    "    \"\"\"\n",
    "    Processes a single VCF file and saves it as a CSV file in the output folder.\n",
    "    \"\"\"\n",
    "    # Extract the file name\n",
    "    vcf_file = os.path.basename(vcf_path)\n",
    "\n",
    "    # Read the header\n",
    "    with open(vcf_path, 'r') as f:\n",
    "        for line in f:\n",
    "            if line.startswith('#CHROM'):\n",
    "                header = line.strip().lstrip('#').split('\\t')\n",
    "                break\n",
    "\n",
    "    # Load the VCF data into a DataFrame\n",
    "    df = pd.read_csv(vcf_path, comment='#', sep='\\t', names=header)\n",
    "\n",
    "    # Split the FORMAT column to create new column names\n",
    "    format_columns = df['FORMAT'][0].split(':')  # Use the first row's FORMAT string to split into new column names\n",
    "\n",
    "    # Define a helper function to split and pad/truncate each sample value\n",
    "    def split_sample(sample, expected_len):\n",
    "        values = sample.split(':')\n",
    "        if len(values) > expected_len:\n",
    "            return values[:expected_len]\n",
    "        elif len(values) < expected_len:\n",
    "            return values + [None] * (expected_len - len(values))\n",
    "        return values\n",
    "\n",
    "    # Apply the split_sample function to each row of the 'sample1' column\n",
    "    df_expanded = df['sample1'].apply(lambda x: split_sample(x, len(format_columns))).apply(pd.Series)\n",
    "\n",
    "    # Assign the new columns from the FORMAT split\n",
    "    df_expanded.columns = format_columns\n",
    "\n",
    "    # Combine the original DataFrame with the new expanded columns\n",
    "    df_final = pd.concat([df.drop(columns=['FORMAT', 'sample1']), df_expanded], axis=1)\n",
    "\n",
    "    # Create output CSV filename (same as VCF but with .csv extension)\n",
    "    csv_file_name = os.path.splitext(vcf_file)[0] + '.csv'\n",
    "    output_csv_path = os.path.join(output_folder, csv_file_name)\n",
    "\n",
    "    # Save the DataFrame as a CSV\n",
    "    df_final.to_csv(output_csv_path, index=False)\n",
    "\n",
    "    #print(f\"Processed {vcf_file} and saved as {csv_file_name} in {output_folder}\")\n",
    "\n",
    "\n",
    "def vcf_to_csv(vcf_folder, output_folder, n_preprocess=100):\n",
    "    \"\"\"\n",
    "    Converts VCF files to CSV files using a ThreadPool.\n",
    "    \"\"\"\n",
    "    # Ensure output folder exists\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    # Gather all VCF file paths\n",
    "    vcf_files = [os.path.join(vcf_folder, f) for f in os.listdir(vcf_folder) if f.endswith('.vcf')]\n",
    "\n",
    "    # Create ThreadPool with the specified number of threads\n",
    "    preprocess_pool = ThreadPool(processes=n_preprocess)\n",
    "\n",
    "    # Submit tasks to the pool\n",
    "    results = [\n",
    "        preprocess_pool.apply_async(process_vcf_file, (vcf_path, output_folder))\n",
    "        for vcf_path in vcf_files\n",
    "    ]\n",
    "\n",
    "    # Wait for all tasks to complete\n",
    "    for result in results:\n",
    "        result.get()  # Retrieve exceptions if any\n",
    "\n",
    "    # Close the pool\n",
    "    preprocess_pool.close()\n",
    "    preprocess_pool.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d07249",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "vcf_to_csv(vcf_dir, vcf_csv_dir, n_preprocess=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb8528e",
   "metadata": {},
   "outputs": [],
   "source": [
    "######### GENOTYPE CALLING\n",
    "######### ----------------\n",
    "# Path to the reference file and the directory containing the cell files\n",
    "reference_file_path = config['truth_GM1_GM2_variant']\n",
    "\n",
    "# Load the reference file into a DataFrame\n",
    "reference_df = pd.read_csv(reference_file_path)\n",
    "\n",
    "# Initialize an empty list to store the data\n",
    "master_data = []\n",
    "\n",
    "# Define the mapping for the GT column\n",
    "gt_mapping = {\n",
    "    '0/0': 0,\n",
    "    '0/1': 1,\n",
    "    '1/0': 1,\n",
    "    '1/1': 2\n",
    "}\n",
    "\n",
    "# Loop through each CSV file in the directory\n",
    "for file_name in os.listdir(vcf_csv_dir):\n",
    "    if file_name.endswith('.csv'):  # Only process CSV files\n",
    "        file_path = os.path.join(vcf_csv_dir, file_name)\n",
    "        \n",
    "        # Read the current cell file\n",
    "        cell_df = pd.read_csv(file_path)\n",
    "        \n",
    "        # Merge with reference data on CHROM and POS, keeping only matching rows\n",
    "        matched_df = pd.merge(cell_df[['CHROM', 'POS', 'GT']], reference_df, on=['CHROM', 'POS'], how='right')\n",
    "        \n",
    "        # If there are no matches (NaNs in GT), set GT to 3\n",
    "        matched_df['GT'] = matched_df['GT'].fillna(3)\n",
    "        \n",
    "        # Map the GT column using the defined mapping\n",
    "        matched_df['GT'] = matched_df['GT'].map(gt_mapping).fillna(3).astype(int)\n",
    "        \n",
    "        # Add a column to indicate the file source\n",
    "        matched_df['File name'] = file_name\n",
    "        \n",
    "        # Drop unwanted columns (\"CELL_LINES\" and \"CALL\")\n",
    "        matched_df = matched_df.drop(columns=['CELL_LINES', 'CALL'], errors='ignore')\n",
    "        \n",
    "        # Modify the \"File name\" column to strip off the \".g.csv\" part\n",
    "        matched_df['File name'] = matched_df['File name'].str.replace('.g.csv$', '', regex=True)\n",
    "        \n",
    "        # Append the processed data to the master list\n",
    "        master_data.append(matched_df)\n",
    "\n",
    "# Combine all individual DataFrames into one master DataFrame\n",
    "master_df = pd.concat(master_data, ignore_index=True)\n",
    "master_df['Variant'] = master_df['CHROM'].astype(str) + \":\" + master_df['POS'].astype(str)\n",
    "master_df = master_df.drop(columns = ['CHROM', 'POS'])\n",
    "master_df = master_df.pivot_table(index='File name', columns='Variant', values='GT', aggfunc='first')\n",
    "\n",
    "# Optionally, save the master DataFrame to a CSV file\n",
    "master_df.to_csv(out_folder + '/genotype_compiling.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0264ce40",
   "metadata": {},
   "source": [
    "### Cell-line related metrics: (Jaccard only for now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a94357",
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_index_with_all(df, row_idx):\n",
    "    \"\"\"\n",
    "    Calculate the Jaccard Index between one row and all rows in a dataframe.\n",
    "    \n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The dataframe.\n",
    "        row_idx (int): Index of the row to compare against all rows.\n",
    "\n",
    "    Returns:\n",
    "        pd.Series: Jaccard indices for each row in the dataframe.\n",
    "    \"\"\"\n",
    "    # Select the target row\n",
    "    target_row = df.loc[row_idx]\n",
    "    \n",
    "    # Calculate Jaccard Index for each row\n",
    "    def calculate_jaccard(row):\n",
    "        # Intersection: Values that match between the two rows\n",
    "        intersection = (target_row == row).sum()\n",
    "        # Union: Total length minus the intersection\n",
    "        union = 2*len(row) - intersection\n",
    "        # Jaccard Index (skip division by zero check for now)\n",
    "        return intersection / union\n",
    "\n",
    "    # Apply the Jaccard Index calculation to all rows\n",
    "    jaccard_indices = df.apply(calculate_jaccard, axis=1)\n",
    "    \n",
    "    return jaccard_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8303099f",
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_df['CHROM:POS'] = reference_df['CHROM'] + ':' + reference_df['POS'].astype(str)\n",
    "reference_df = reference_df.drop(['CHROM', 'POS'], axis=1)\n",
    "pivot_df = reference_df.pivot(index='CELL_LINES', columns='CHROM:POS', values='CALL')\n",
    "pivot_df = pivot_df.fillna(0)\n",
    "ref_df = pivot_df\n",
    "ref_df = pd.DataFrame(ref_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee82d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "snp_df_combine = pd.concat([ref_df, master_df])\n",
    "snp_df_combine_drop = snp_df_combine.drop(['chr4:54736599', 'chr7:148808972'], axis=1)\n",
    "snp_df_combine\n",
    "cell_line_1_jaccard = jaccard_index_with_all(snp_df_combine_drop, 'GM12878')\n",
    "cell_line_2_jaccard = jaccard_index_with_all(snp_df_combine_drop, 'GM24385')\n",
    "jaccard_df = pd.DataFrame({'Jaccard_GM12878': cell_line_1_jaccard,\n",
    "                          'Jaccard_GM24385': cell_line_2_jaccard})\n",
    "jaccard_df.to_csv(out_folder + '/Jaccard_metrics.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
